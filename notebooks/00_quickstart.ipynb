{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AstroLlama Quick Start\n",
    "\n",
    "This notebook walks you through:\n",
    "1. Setting up your environment\n",
    "2. Testing the tools\n",
    "3. Preparing training data\n",
    "4. Running the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q langchain langchain-openai astroquery astropy photutils together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your API keys\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"your-together-api-key\"  # Get from together.ai\n",
    "os.environ[\"ADS_DEV_KEY\"] = \"your-ads-token\"  # Get from ui.adsabs.harvard.edu/user/settings/token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Individual Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test catalog query\n",
    "from astroquery.gaia import Gaia\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "# Query Gaia for M13\n",
    "m13 = SkyCoord.from_name('M13')\n",
    "print(f\"M13 coordinates: RA={m13.ra.deg:.4f}, Dec={m13.dec.deg:.4f}\")\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT TOP 100 source_id, ra, dec, phot_g_mean_mag, bp_rp\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE CONTAINS(\n",
    "    POINT('ICRS', ra, dec),\n",
    "    CIRCLE('ICRS', {m13.ra.deg}, {m13.dec.deg}, 0.1)\n",
    ") = 1\n",
    "AND phot_g_mean_mag < 18\n",
    "\"\"\"\n",
    "\n",
    "job = Gaia.launch_job(query)\n",
    "result = job.get_results()\n",
    "print(f\"Found {len(result)} sources\")\n",
    "result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.scatter(result['bp_rp'], result['phot_g_mean_mag'], s=5, alpha=0.7)\n",
    "ax.set_xlabel('BP - RP')\n",
    "ax.set_ylabel('G magnitude')\n",
    "ax.set_title('M13 Color-Magnitude Diagram')\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ADS search\n",
    "from astroquery.nasa_ads import ADS\n",
    "\n",
    "ADS.TOKEN = os.environ.get('ADS_DEV_KEY')\n",
    "ADS.NROWS = 5\n",
    "ADS.ADS_FIELDS = ['bibcode', 'title', 'author', 'year', 'citation_count']\n",
    "\n",
    "results = ADS.query_simple('JWST exoplanet atmosphere year:2023-2025')\n",
    "for paper in results:\n",
    "    print(f\"- {paper['title'][0][:60]}...\")\n",
    "    print(f\"  {paper['author'][0]} et al. ({paper['year']}) - {paper['citation_count']} citations\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Data\n",
    "\n",
    "Your training data should be in JSONL format with chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example training data format\n",
    "training_examples = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert astronomy research assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"How do I select main sequence stars from a CMD?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"To select main sequence stars from a color-magnitude diagram...\"}\n",
    "        ]\n",
    "    },\n",
    "    # Add more examples...\n",
    "]\n",
    "\n",
    "# Save to JSONL\n",
    "with open('../data/training/my_training_data.jsonl', 'w') as f:\n",
    "    for example in training_examples:\n",
    "        f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "print(f\"Saved {len(training_examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Agent (using base model first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with Together.ai API (no fine-tuning yet)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Import our tools\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.tools.astronomy_tools import get_tools\n",
    "\n",
    "# Initialize LLM (using base Llama first, switch to fine-tuned later)\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=os.environ[\"TOGETHER_API_KEY\"],\n",
    "    model=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Get tools\n",
    "tools = get_tools()\n",
    "print(f\"Available tools: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "SYSTEM_PROMPT = \"\"\"You are AstroLlama, an expert astronomy research assistant.\n",
    "You can query catalogs, create plots, search literature, and analyze data.\n",
    "Use the available tools to help answer questions.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    max_iterations=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent!\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": \"Query Gaia DR3 for sources within 5 arcmin of M31 and create a CMD\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESPONSE:\")\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test: literature search\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": \"Find recent papers about stellar streams in the Milky Way halo and give me the citations\"\n",
    "})\n",
    "\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tune Your Model\n",
    "\n",
    "Once you have enough training data (recommended: 500-5000 examples), fine-tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Fine-tune using Together.ai (easiest)\n",
    "!python ../scripts/fine_tune.py --together-finetune --train-file ../data/training/combined_train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Fine-tune locally with QLoRA (requires 2x A100 or similar)\n",
    "# !python ../scripts/fine_tune.py --local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use Your Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fine-tuning completes, update the model name\n",
    "FINE_TUNED_MODEL = \"your-username/astro-llama-70b\"  # Replace with your model\n",
    "\n",
    "llm_finetuned = ChatOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=os.environ[\"TOGETHER_API_KEY\"],\n",
    "    model=FINE_TUNED_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Recreate agent with fine-tuned model\n",
    "agent_ft = create_tool_calling_agent(llm_finetuned, tools, prompt)\n",
    "agent_executor_ft = AgentExecutor(agent=agent_ft, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Estimates\n",
    "\n",
    "| Task | Estimated Cost |\n",
    "|------|---------------|\n",
    "| Fine-tuning (Together.ai, 1000 examples) | ~$50-100 |\n",
    "| API usage (1000 queries/month) | ~$2-5 |\n",
    "| Total first month | ~$55-105 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
